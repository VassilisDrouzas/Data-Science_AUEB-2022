{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "533dbfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import csv\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB,BernoulliNB\n",
    "from sklearn import tree\n",
    "import xgboost as xg\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8207c29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499\n",
      "Number of edges: 1091955\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "G = nx.read_edgelist('edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "nodes = list(G.nodes())\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "print('Number of nodes:', n)\n",
    "print('Number of edges:', m)\n",
    "print(nx.is_connected(G))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "176c073b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 10000 nodes and 56226 edges\n"
     ]
    }
   ],
   "source": [
    "subgraph=G\n",
    "for i in range (10000,n):\n",
    "    subgraph.remove_node(i)\n",
    "    \n",
    "print(nx.info(subgraph))\n",
    "\n",
    "#nx.draw(subgraph)\n",
    "\n",
    "#FIND GIANT COMPONENT\n",
    "#components = nx.connected_components(G)\n",
    "#largest_component = max(components, key=len)\n",
    "\n",
    "#subgraph = G.subgraph(largest_component)\n",
    "#diameter = nx.diameter(subgraph)\n",
    "#print(\"Network diameter of largest component:\", diameter)\n",
    "\n",
    "#triadic_closure = nx.transitivity(G)\n",
    "#print(\"Triadic closure:\", triadic_closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea10b1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G density:  0.0011246324632463247\n",
      "sub density:  0.0011246324632463247\n",
      "Shortest path between these two is: [154, 66, 245, 4860, 6551]\n",
      "Triadic closure: 0.02696968287398654\n",
      "(1, 3)\n",
      "(1, 7)\n",
      "(1, 15)\n",
      "(1, 17)\n",
      "(1, 19)\n",
      "(2, 27)\n",
      "(2, 29)\n",
      "(2, 46)\n",
      "(2, 56)\n",
      "(7, 8)\n",
      "(43, 412)\n",
      "(43, 428)\n",
      "(4, 5202)\n",
      "(798, 4014)\n",
      "(798, 4137)\n",
      "(798, 4141)\n",
      "(798, 4179)\n",
      "(798, 4301)\n",
      "(798, 4431)\n",
      "(798, 4541)\n",
      "(798, 4554)\n",
      "(798, 4584)\n",
      "(407, 47)\n",
      "(407, 479)\n",
      "(407, 480)\n",
      "(407, 484)\n",
      "(407, 488)\n",
      "(407, 490)\n",
      "(407, 492)\n",
      "(407, 497)\n",
      "(407, 499)\n",
      "(407, 501)\n",
      "(407, 508)\n",
      "(407, 510)\n",
      "(407, 512)\n",
      "(407, 513)\n",
      "(407, 517)\n",
      "(407, 518)\n",
      "(407, 521)\n",
      "(407, 526)\n",
      "(407, 527)\n",
      "(407, 534)\n",
      "(407, 535)\n",
      "(407, 536)\n",
      "(407, 537)\n",
      "(407, 540)\n",
      "(407, 546)\n",
      "(407, 548)\n",
      "(407, 549)\n",
      "(407, 553)\n",
      "(407, 554)\n",
      "(407, 561)\n",
      "(407, 562)\n",
      "(407, 566)\n",
      "(407, 567)\n",
      "(407, 569)\n",
      "(407, 578)\n",
      "(407, 581)\n",
      "(407, 582)\n",
      "(407, 589)\n",
      "(407, 596)\n",
      "(407, 599)\n",
      "(407, 605)\n",
      "(407, 607)\n",
      "(407, 609)\n",
      "(407, 613)\n",
      "(407, 614)\n",
      "(407, 617)\n",
      "(407, 621)\n",
      "(407, 623)\n",
      "(407, 625)\n",
      "(407, 634)\n",
      "(407, 636)\n",
      "(407, 637)\n",
      "(407, 638)\n",
      "(407, 643)\n",
      "(407, 650)\n",
      "(407, 652)\n",
      "(407, 657)\n",
      "(407, 658)\n",
      "(407, 661)\n",
      "(407, 664)\n",
      "(407, 672)\n",
      "(407, 675)\n",
      "(407, 676)\n",
      "(407, 682)\n",
      "(407, 684)\n",
      "(407, 687)\n",
      "(407, 689)\n",
      "(407, 694)\n",
      "(407, 696)\n",
      "(407, 699)\n",
      "(407, 701)\n",
      "(407, 704)\n",
      "(407, 709)\n",
      "(407, 715)\n",
      "(407, 716)\n",
      "(407, 718)\n",
      "(407, 720)\n",
      "(407, 724)\n",
      "(407, 727)\n",
      "(407, 728)\n",
      "(407, 730)\n",
      "(407, 732)\n",
      "(407, 742)\n",
      "(407, 760)\n",
      "(407, 764)\n",
      "(407, 766)\n",
      "(407, 771)\n",
      "(407, 772)\n",
      "(407, 779)\n",
      "(407, 784)\n",
      "(407, 785)\n",
      "(407, 786)\n",
      "(407, 789)\n",
      "(407, 791)\n",
      "(405, 460)\n",
      "(405, 462)\n",
      "(1253, 369)\n",
      "(4611, 1625)\n",
      "(6359, 4860)\n",
      "(5411, 7722)\n",
      "(4859, 4880)\n",
      "(4859, 4886)\n",
      "(4859, 4933)\n",
      "(4859, 4934)\n",
      "(4859, 4941)\n",
      "(4859, 4988)\n",
      "(4859, 5106)\n",
      "(4859, 5129)\n",
      "(4859, 5159)\n",
      "(4859, 5167)\n",
      "(4859, 5285)\n",
      "(4859, 5292)\n",
      "(4859, 5295)\n",
      "(4859, 5309)\n",
      "(4859, 5341)\n",
      "(4859, 5552)\n",
      "(4859, 5616)\n",
      "(4859, 5645)\n",
      "(4859, 5658)\n",
      "(4859, 5676)\n",
      "(4859, 5686)\n",
      "(4859, 5735)\n",
      "(4859, 5748)\n",
      "(4859, 5826)\n",
      "(4859, 5830)\n",
      "(4859, 5833)\n",
      "(4859, 5839)\n",
      "(4859, 5842)\n",
      "(4859, 5907)\n",
      "(4859, 5980)\n",
      "(4859, 6031)\n",
      "(6758, 4860)\n",
      "(9530, 9512)\n",
      "(7082, 4860)\n",
      "(4861, 7250)\n",
      "(4861, 7266)\n",
      "(4861, 7292)\n",
      "(5444, 5720)\n",
      "(6410, 6284)\n",
      "(5413, 6431)\n",
      "(4860, 6046)\n",
      "(4860, 6048)\n",
      "(4860, 6052)\n",
      "(4860, 6069)\n",
      "(4860, 6076)\n",
      "(4860, 6077)\n",
      "(4860, 6081)\n",
      "(4860, 6084)\n",
      "(4860, 6090)\n",
      "(4860, 6093)\n",
      "(4860, 6097)\n",
      "(4860, 6100)\n",
      "(4860, 6106)\n",
      "(4860, 6108)\n",
      "(4860, 6113)\n",
      "(4860, 6122)\n",
      "(4860, 6125)\n",
      "(4860, 6146)\n",
      "(4860, 6162)\n",
      "(4860, 6195)\n",
      "(4860, 6206)\n",
      "(4860, 6215)\n",
      "(4860, 6225)\n",
      "(4860, 6231)\n",
      "(4860, 6234)\n",
      "(4860, 6243)\n",
      "(4860, 6252)\n",
      "(4860, 6257)\n",
      "(4860, 6258)\n",
      "(4860, 6259)\n",
      "(4860, 6265)\n",
      "(4860, 6276)\n",
      "(4860, 6283)\n",
      "(4860, 6284)\n",
      "(4860, 6285)\n",
      "(4860, 6296)\n",
      "(4860, 6302)\n",
      "(4860, 6313)\n",
      "(4860, 6314)\n",
      "(4860, 6315)\n",
      "(4860, 6317)\n",
      "(4860, 6322)\n",
      "(4860, 6335)\n",
      "(4860, 6353)\n",
      "(4860, 6354)\n",
      "(4860, 6360)\n",
      "(4860, 6378)\n",
      "(4860, 6380)\n",
      "(4860, 6381)\n",
      "(4860, 6382)\n",
      "(4860, 6392)\n",
      "(4860, 6394)\n",
      "(4860, 6395)\n",
      "(4860, 6399)\n",
      "(4860, 6401)\n",
      "(4860, 6402)\n",
      "(4860, 6403)\n",
      "(4860, 6413)\n",
      "(4860, 6417)\n",
      "(4860, 6419)\n",
      "(4860, 6420)\n",
      "(4860, 6424)\n",
      "(4860, 6432)\n",
      "(4860, 6437)\n",
      "(4860, 6443)\n",
      "(4860, 6450)\n",
      "(4860, 6466)\n",
      "(4860, 6470)\n",
      "(4860, 6475)\n",
      "(4860, 6479)\n",
      "(4860, 6483)\n",
      "(4860, 6487)\n",
      "(4860, 6491)\n",
      "(4860, 6534)\n",
      "(4860, 6536)\n",
      "(4860, 6537)\n",
      "(4860, 6544)\n",
      "(4860, 6551)\n",
      "(4860, 6566)\n",
      "(4860, 6578)\n",
      "(4860, 6602)\n",
      "(4860, 6620)\n",
      "(4860, 6625)\n",
      "(4860, 6638)\n",
      "(4860, 6641)\n",
      "(4860, 6643)\n",
      "(4860, 6646)\n",
      "(4860, 6653)\n",
      "(4860, 6683)\n",
      "(4860, 6693)\n",
      "(4860, 6695)\n",
      "(4860, 6703)\n",
      "(4860, 6708)\n",
      "(4860, 6727)\n",
      "(4860, 6729)\n",
      "(4860, 6731)\n",
      "(4860, 6747)\n",
      "(4860, 6750)\n",
      "(4860, 6787)\n",
      "(4860, 6802)\n",
      "(4860, 6807)\n",
      "(4860, 6811)\n",
      "(4860, 6814)\n",
      "(4860, 6823)\n",
      "(4860, 6846)\n",
      "(4860, 6853)\n",
      "(4860, 6855)\n",
      "(4860, 6865)\n",
      "(4860, 6867)\n",
      "(4860, 6881)\n",
      "(4860, 6883)\n",
      "(4860, 6886)\n",
      "(4860, 6889)\n",
      "(4860, 6897)\n",
      "(4860, 6904)\n",
      "(4860, 6910)\n",
      "(4860, 6945)\n",
      "(4860, 6948)\n",
      "(4860, 6955)\n",
      "(4860, 6961)\n",
      "(4860, 6971)\n",
      "(4860, 6973)\n",
      "(4860, 6976)\n",
      "(4860, 6978)\n",
      "(4860, 6980)\n",
      "(4860, 6981)\n",
      "(4860, 6987)\n",
      "(4860, 6998)\n",
      "(4860, 7008)\n",
      "(4860, 7009)\n",
      "(4860, 7017)\n",
      "(4860, 7021)\n",
      "(4860, 7022)\n",
      "(4860, 7023)\n",
      "(4860, 7024)\n",
      "(4860, 7032)\n",
      "(4860, 7036)\n",
      "(4860, 7041)\n",
      "(4860, 7050)\n",
      "(4860, 7059)\n",
      "(4860, 7071)\n",
      "(4860, 7075)\n",
      "(4860, 7076)\n",
      "(4860, 7089)\n",
      "(4860, 7102)\n",
      "(4860, 7114)\n",
      "(4860, 7116)\n",
      "(4860, 7118)\n",
      "(4860, 7145)\n",
      "(4860, 7146)\n",
      "(4860, 7148)\n",
      "(4860, 7152)\n",
      "(4860, 7154)\n",
      "(4860, 7157)\n",
      "(4860, 7163)\n",
      "(4860, 7164)\n",
      "(4860, 7168)\n",
      "(4860, 7174)\n",
      "(4860, 7175)\n",
      "(4860, 7184)\n",
      "(4860, 7185)\n",
      "(4860, 7192)\n",
      "(4860, 7193)\n",
      "(4860, 7197)\n",
      "(4860, 7198)\n",
      "(4860, 7200)\n",
      "(4860, 7201)\n",
      "(5425, 5041)\n",
      "(323, 4834)\n",
      "(3224, 369)\n",
      "(2442, 1571)\n",
      "(684, 622)\n",
      "(4815, 7005)\n",
      "(6134, 763)\n",
      "(6134, 6433)\n",
      "(402, 408)\n",
      "(403, 441)\n",
      "(404, 444)\n",
      "(404, 446)\n",
      "(7665, 7630)\n",
      "(4833, 7173)\n",
      "(6388, 6780)\n",
      "(66, 87)\n",
      "(66, 90)\n",
      "(66, 114)\n",
      "(66, 122)\n",
      "(66, 127)\n",
      "(66, 157)\n",
      "(66, 185)\n",
      "(66, 194)\n",
      "(66, 197)\n",
      "(66, 213)\n",
      "(66, 244)\n",
      "(66, 249)\n",
      "(66, 256)\n",
      "(66, 265)\n",
      "(66, 271)\n",
      "(66, 304)\n",
      "(66, 310)\n",
      "(66, 313)\n",
      "(66, 319)\n",
      "(66, 320)\n",
      "(66, 326)\n",
      "(69, 360)\n",
      "(73, 375)\n",
      "(369, 818)\n",
      "(369, 823)\n",
      "(369, 830)\n",
      "(369, 842)\n",
      "(369, 859)\n",
      "(369, 884)\n",
      "(369, 907)\n",
      "(369, 909)\n",
      "(369, 933)\n",
      "(369, 944)\n",
      "(369, 955)\n",
      "(369, 960)\n",
      "(369, 967)\n",
      "(369, 983)\n",
      "(369, 984)\n",
      "(369, 999)\n",
      "(369, 1010)\n",
      "(369, 1016)\n",
      "(369, 1028)\n",
      "(369, 1048)\n",
      "(369, 1059)\n",
      "(369, 1060)\n",
      "(369, 1082)\n",
      "(369, 1102)\n",
      "(369, 1143)\n",
      "(369, 1149)\n",
      "(369, 1226)\n",
      "(369, 1229)\n",
      "(369, 1236)\n",
      "(369, 1243)\n",
      "(369, 1277)\n",
      "(369, 1305)\n",
      "(369, 1309)\n",
      "(369, 1316)\n",
      "(369, 1344)\n",
      "(369, 1358)\n",
      "(369, 1359)\n",
      "(369, 1363)\n",
      "(369, 1388)\n",
      "(369, 1396)\n",
      "(369, 1435)\n",
      "(369, 1438)\n",
      "(369, 1452)\n",
      "(369, 1467)\n",
      "(369, 1482)\n",
      "(369, 1520)\n",
      "(369, 1540)\n",
      "(369, 1562)\n",
      "(369, 1595)\n",
      "(369, 1607)\n",
      "(369, 1622)\n",
      "(369, 1646)\n",
      "(369, 1669)\n",
      "(369, 1672)\n",
      "(369, 1683)\n",
      "(369, 1688)\n",
      "(369, 1714)\n",
      "(369, 1719)\n",
      "(369, 1727)\n",
      "(369, 1738)\n",
      "(369, 1742)\n",
      "(369, 1755)\n",
      "(369, 1781)\n",
      "(369, 1800)\n",
      "(369, 1808)\n",
      "(369, 1936)\n",
      "(369, 1979)\n",
      "(369, 2012)\n",
      "(369, 2023)\n",
      "(369, 2033)\n",
      "(369, 2050)\n",
      "(369, 2056)\n",
      "(369, 2073)\n",
      "(369, 2090)\n",
      "(369, 2093)\n",
      "(369, 2165)\n",
      "(369, 2171)\n",
      "(369, 2187)\n",
      "(369, 2202)\n",
      "(369, 2222)\n",
      "(369, 2230)\n",
      "(369, 2253)\n",
      "(369, 2277)\n",
      "(369, 2288)\n",
      "(369, 2325)\n",
      "(369, 2326)\n",
      "(369, 2361)\n",
      "(369, 2453)\n",
      "(369, 2466)\n",
      "(369, 2508)\n",
      "(369, 2536)\n",
      "(369, 2552)\n",
      "(369, 2553)\n",
      "(369, 2585)\n",
      "(369, 2593)\n",
      "(369, 2607)\n",
      "(369, 2623)\n",
      "(369, 2638)\n",
      "(369, 2646)\n",
      "(369, 2654)\n",
      "(369, 2656)\n",
      "(369, 2671)\n",
      "(369, 2702)\n",
      "(369, 2706)\n",
      "(369, 2707)\n",
      "(369, 2717)\n",
      "(369, 2732)\n",
      "(369, 2735)\n",
      "(369, 2775)\n",
      "(369, 2795)\n",
      "(369, 2834)\n",
      "(369, 2838)\n",
      "(369, 2840)\n",
      "(369, 2843)\n",
      "(369, 2854)\n",
      "(369, 2868)\n",
      "(369, 2886)\n",
      "(369, 2892)\n",
      "(369, 2906)\n",
      "(369, 2912)\n",
      "(369, 2921)\n",
      "(369, 2930)\n",
      "(369, 2936)\n",
      "(369, 2939)\n",
      "(369, 2946)\n",
      "(369, 2962)\n",
      "(369, 2973)\n",
      "(369, 2975)\n",
      "(369, 2980)\n",
      "(369, 2985)\n",
      "(369, 2990)\n",
      "(369, 2993)\n",
      "(369, 3006)\n",
      "(369, 3015)\n",
      "(369, 3021)\n",
      "(369, 3029)\n",
      "(369, 3034)\n",
      "(369, 3046)\n",
      "(369, 3058)\n",
      "(369, 3082)\n",
      "(369, 3085)\n",
      "(369, 3099)\n",
      "(369, 3100)\n",
      "(369, 3112)\n",
      "(369, 3120)\n",
      "(369, 3132)\n",
      "(369, 3140)\n",
      "(369, 3149)\n",
      "(369, 3154)\n",
      "(369, 3157)\n",
      "(369, 3165)\n",
      "(369, 3187)\n",
      "(369, 3192)\n",
      "(369, 3194)\n",
      "(369, 3225)\n",
      "(369, 3231)\n",
      "(369, 3239)\n",
      "(369, 3249)\n",
      "(369, 3269)\n",
      "(369, 3272)\n",
      "(369, 3302)\n",
      "(369, 3311)\n",
      "(369, 3318)\n",
      "(369, 3319)\n",
      "(369, 3322)\n",
      "(369, 3335)\n",
      "(369, 3336)\n",
      "(369, 3340)\n",
      "(369, 3353)\n",
      "(369, 3356)\n",
      "(369, 3379)\n",
      "(369, 3380)\n",
      "(369, 3383)\n",
      "(369, 3384)\n",
      "(369, 3387)\n",
      "(369, 3421)\n",
      "(369, 3426)\n",
      "(369, 3435)\n",
      "(369, 3462)\n",
      "(369, 3474)\n",
      "(369, 3475)\n",
      "(369, 3490)\n",
      "(369, 3513)\n",
      "(369, 3518)\n",
      "(369, 3519)\n",
      "(369, 3549)\n",
      "(369, 3566)\n",
      "(369, 3581)\n",
      "(369, 3582)\n",
      "(369, 3599)\n",
      "(369, 3618)\n",
      "(369, 3628)\n",
      "(369, 3634)\n",
      "(369, 3635)\n",
      "(369, 3636)\n",
      "(369, 3644)\n",
      "(369, 3647)\n",
      "(369, 3665)\n",
      "(369, 3670)\n",
      "(369, 3675)\n",
      "(369, 3695)\n",
      "(369, 3708)\n",
      "(369, 3710)\n",
      "(369, 3711)\n",
      "(369, 3716)\n",
      "(369, 3718)\n",
      "(369, 3722)\n",
      "(369, 3740)\n",
      "(369, 3751)\n",
      "(369, 3753)\n",
      "(369, 3762)\n",
      "(369, 3773)\n",
      "(369, 3779)\n",
      "(369, 3801)\n",
      "(369, 3831)\n",
      "(369, 3839)\n",
      "(369, 3847)\n",
      "(369, 3856)\n",
      "(369, 3858)\n",
      "(369, 3883)\n",
      "(369, 3887)\n",
      "(7316, 7395)\n",
      "(7316, 7397)\n",
      "(7316, 7400)\n",
      "(8921, 2179)\n",
      "(8921, 8751)\n",
      "(161, 3611)\n",
      "(7757, 7824)\n",
      "(7757, 7832)\n",
      "(7757, 7834)\n",
      "(7757, 7836)\n",
      "(7757, 7837)\n",
      "(7757, 7859)\n",
      "(7757, 7860)\n",
      "(7757, 7861)\n",
      "(7757, 7896)\n",
      "(7757, 7903)\n",
      "(7757, 7905)\n",
      "(7757, 7918)\n",
      "(7757, 7943)\n",
      "(8005, 7795)\n",
      "(796, 3922)\n",
      "(796, 3936)\n",
      "(796, 3940)\n",
      "(796, 3960)\n",
      "(1164, 1687)\n",
      "(8492, 8491)\n",
      "(8492, 8510)\n",
      "(8492, 8547)\n",
      "(8492, 8861)\n",
      "(8492, 8979)\n",
      "(8492, 9029)\n",
      "(8492, 9041)\n",
      "(8492, 9104)\n",
      "(8492, 9140)\n",
      "(8492, 9143)\n",
      "(8492, 9151)\n",
      "(8492, 9177)\n",
      "(8492, 9208)\n",
      "(8492, 9217)\n",
      "(8492, 9232)\n",
      "(804, 4825)\n",
      "(804, 4849)\n",
      "(7760, 8046)\n",
      "(7760, 8063)\n",
      "(7760, 8067)\n",
      "(7760, 8070)\n",
      "(7760, 8080)\n",
      "(7760, 8089)\n",
      "(7760, 8090)\n",
      "(7760, 8091)\n",
      "(7760, 8099)\n",
      "(7760, 8105)\n",
      "(7760, 8106)\n",
      "(7760, 8107)\n",
      "(7760, 8115)\n",
      "(7760, 8117)\n",
      "(7760, 8118)\n",
      "(7760, 8123)\n",
      "(7760, 8124)\n",
      "(7760, 8129)\n",
      "(7758, 7966)\n",
      "(7758, 7975)\n",
      "(7758, 7988)\n",
      "(7758, 8000)\n",
      "(7758, 8020)\n",
      "(2428, 8791)\n",
      "(1830, 9782)\n",
      "(8264, 8347)\n",
      "(8264, 8351)\n",
      "(8264, 8353)\n",
      "(8264, 8355)\n",
      "(8264, 8373)\n",
      "(8264, 8399)\n",
      "(8264, 8410)\n",
      "(8264, 8425)\n",
      "(8264, 8426)\n",
      "(7497, 7399)\n",
      "(9734, 9845)\n",
      "(936, 3834)\n",
      "(7319, 7429)\n",
      "(7319, 7438)\n",
      "(7319, 7446)\n",
      "(7319, 7455)\n",
      "(7319, 7460)\n",
      "(7319, 7525)\n",
      "(7319, 7550)\n",
      "(7319, 7570)\n",
      "(7319, 7577)\n",
      "(7319, 7580)\n",
      "(7319, 7581)\n",
      "(9513, 9580)\n",
      "(9513, 9663)\n",
      "(9513, 9673)\n",
      "(9513, 9693)\n",
      "(7754, 7806)\n",
      "(7759, 8026)\n",
      "(7759, 8028)\n",
      "(802, 4714)\n",
      "(802, 4720)\n",
      "(802, 4727)\n",
      "(802, 4744)\n",
      "(802, 4747)\n",
      "(802, 4748)\n",
      "(802, 4753)\n",
      "(802, 4758)\n",
      "(1181, 3226)\n",
      "(1476, 9942)\n",
      "(1540, 3717)\n",
      "(1742, 1564)\n",
      "(1762, 8379)\n",
      "(799, 4663)\n",
      "(2220, 3433)\n",
      "(2343, 9445)\n",
      "(3013, 1839)\n",
      "(3555, 3825)\n",
      "(3868, 3443)\n",
      "(3887, 9510)\n",
      "(7312, 7321)\n",
      "(1292, 9715)\n",
      "(1292, 9727)\n",
      "(1292, 9776)\n",
      "(1292, 9790)\n",
      "(1292, 9803)\n",
      "(1292, 9899)\n",
      "(1292, 9907)\n",
      "(1292, 9922)\n",
      "(1292, 9928)\n",
      "(1292, 9929)\n",
      "(1292, 9971)\n",
      "(7313, 7348)\n",
      "(7318, 7409)\n",
      "(7318, 7411)\n",
      "(8877, 9128)\n",
      "(7116, 4227)\n",
      "(4775, 803)\n",
      "(7737, 7716)\n",
      "(7866, 8014)\n",
      "(7887, 7551)\n",
      "(797, 3973)\n",
      "(797, 3977)\n",
      "(797, 3978)\n",
      "(797, 3992)\n",
      "(2100, 2947)\n",
      "(379, 378)\n",
      "(379, 381)\n",
      "(379, 382)\n",
      "(379, 391)\n",
      "(379, 393)\n",
      "(9503, 9502)\n",
      "(9505, 9502)\n",
      "(7753, 7787)\n",
      "(801, 4701)\n",
      "(801, 4708)\n",
      "(803, 4768)\n",
      "(803, 4773)\n",
      "(803, 4776)\n",
      "(803, 4790)\n",
      "(803, 4792)\n",
      "(803, 4800)\n",
      "(803, 4803)\n",
      "(803, 4807)\n",
      "(803, 4810)\n",
      "(803, 4811)\n",
      "(803, 4814)\n",
      "(9810, 9857)\n",
      "(5846, 5822)\n",
      "(9337, 9256)\n",
      "(7682, 7631)\n",
      "(9254, 9263)\n",
      "(9254, 9268)\n",
      "(8340, 8263)\n",
      "(4695, 3544)\n",
      "(9349, 9256)\n",
      "(9256, 9330)\n",
      "(9256, 9352)\n",
      "(9256, 9358)\n",
      "(9256, 9381)\n",
      "(8262, 8287)\n",
      "(7052, 9981)\n",
      "(4857, 4858)\n",
      "(6231, 6235)\n",
      "(6538, 6226)\n",
      "(6617, 6772)\n",
      "(6797, 6426)\n",
      "(4453, 9506)\n",
      "(8152, 8132)\n",
      "(9754, 3641)\n",
      "(2268, 439)\n",
      "(9510, 9508)\n",
      "(378, 380)\n",
      "(8237, 8168)\n",
      "(8167, 8221)\n",
      "(8167, 8225)\n",
      "(9506, 9502)\n",
      "(7629, 7648)\n",
      "(7630, 7667)\n",
      "(6267, 7165)\n",
      "(7631, 7679)\n",
      "(7631, 7688)\n",
      "(7631, 7711)\n",
      "(8158, 8132)\n",
      "(7628, 7638)\n",
      "(9486, 9412)\n",
      "(8265, 8431)\n",
      "(8265, 8439)\n",
      "(8263, 8289)\n",
      "(8263, 8293)\n",
      "(8263, 8313)\n",
      "(8263, 8314)\n",
      "(8263, 8317)\n",
      "(8263, 8319)\n",
      "(8263, 8329)\n",
      "(8263, 8332)\n",
      "(8263, 8338)\n",
      "(8263, 8339)\n",
      "(9483, 9411)\n",
      "(8267, 8469)\n",
      "(8267, 8472)\n",
      "(8267, 8489)\n",
      "(8232, 8168)\n",
      "(8234, 8168)\n",
      "(8239, 8168)\n",
      "(8238, 8168)\n",
      "(8307, 8315)\n",
      "(9255, 9275)\n",
      "(9255, 9278)\n",
      "(9255, 9289)\n",
      "(9260, 9261)\n",
      "(8168, 8229)\n",
      "(8168, 8243)\n",
      "(8168, 8250)\n",
      "(8168, 8254)\n",
      "(8168, 8255)\n",
      "(8168, 8256)\n",
      "(8168, 8258)\n",
      "(8183, 8165)\n",
      "(8159, 8132)\n",
      "(9508, 8156)\n",
      "(9508, 9507)\n",
      "(9508, 9509)\n",
      "(9508, 9511)\n",
      "(9502, 9501)\n",
      "(9502, 9504)\n",
      "(9253, 9261)\n",
      "(9464, 9411)\n",
      "(9435, 9409)\n",
      "(4853, 4854)\n",
      "(4854, 4856)\n",
      "(8150, 8132)\n",
      "(8145, 8132)\n",
      "(8132, 8139)\n",
      "(8132, 8142)\n",
      "(8132, 8146)\n",
      "(8132, 8148)\n",
      "(8132, 8149)\n",
      "(8132, 8157)\n",
      "(8133, 8163)\n",
      "(8165, 8170)\n",
      "(8165, 8172)\n",
      "(8165, 8182)\n",
      "(9409, 9432)\n",
      "(9409, 9434)\n",
      "(9411, 9468)\n",
      "------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37302\n",
      "-0.0812\n"
     ]
    }
   ],
   "source": [
    "#Find density on G and on the subgraph(they prove to be the same)\n",
    "G_density=nx.density(G)\n",
    "print(\"G density: \",G_density)\n",
    "\n",
    "sub_density=nx.density(subgraph)\n",
    "print(\"sub density: \",sub_density)\n",
    "\n",
    "#Finds all possible paths and chooses the shortest one\n",
    "short_path=nx.shortest_path(G,source=154,target=6551)\n",
    "print(\"Shortest path between these two is:\", short_path)\n",
    "\n",
    "triadic_closure = nx.transitivity(G)\n",
    "print(\"Triadic closure:\", triadic_closure)\n",
    "\n",
    "bridges=list(nx.bridges(subgraph))\n",
    "\n",
    "for bridge in bridges:\n",
    "    print(bridge)\n",
    "    \n",
    "print(\"------------\")\n",
    "#local_bridges=list(nx.local_bridges(subgraph))\n",
    "\n",
    "#for local in local_bridges:\n",
    "    #print(local)\n",
    "\n",
    "cliques = list(nx.find_cliques(G))\n",
    "\n",
    "clique_number = len(list(cliques))\n",
    "print(clique_number)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"%.4f\"% nx.degree_assortativity_coefficient(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb9002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find degree \n",
    "degree_dict = dict(G.degree(G.nodes()))\n",
    "nx.set_node_attributes(G, degree_dict, 'degree')\n",
    "print(G.nodes[2112])\n",
    "\n",
    "from operator import itemgetter\n",
    "sorted_degree = sorted(degree_dict.items(), key=itemgetter(1), reverse=True)\n",
    "print(\"Top 20 nodes by degree:\")\n",
    "for d in sorted_degree[:20]:\n",
    "    print(d)\n",
    "    \n",
    "    \n",
    "betweenness_dict = nx.betweenness_centrality(subgraph) # Run betweenness centrality\n",
    "eigenvector_dict = nx.eigenvector_centrality(subgraph) # Run eigenvector centrality\n",
    "\n",
    "# Assign each to an attribute in your network\n",
    "nx.set_node_attributes(subgraph, betweenness_dict, 'betweenness')\n",
    "nx.set_node_attributes(subgraph, eigenvector_dict, 'eigenvector')\n",
    "\n",
    "sorted_betweenness = sorted(betweenness_dict.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "print(\"Top 20 nodes by betweenness centrality:\")\n",
    "for b in sorted_betweenness[:20]:\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769506ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the communities in the graph\n",
    "from networkx.algorithms import community\n",
    "communities = community.greedy_modularity_communities(subgraph)\n",
    "print( len(communities))\n",
    "\n",
    "    \n",
    "for i,c in enumerate(communities): # Loop through the list of communities\n",
    "   # if len(c) > 2: # Filter out modularity classes with 2 or fewer nodes\n",
    "        print('Class '+str(i)+':',len(c), list(c)) # Print out the classes and their members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ca62415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the abstract of each paper\n",
    "abstracts = dict()\n",
    "with open('abstracts.txt', 'r',encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        node, abstract = line.split('|--|')\n",
    "        abstracts[int(node)] = abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e27ddc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'development', 'of', 'an', 'automated', 'system', 'for', 'the', 'quality', 'assessment', 'of', 'aerodrome', 'ground', 'lighting', '(', 'agl', ')', ',', 'in', 'accordance', 'with', 'associated', 'standards', 'and', 'recommendations', ',', 'is', 'presented', '.'], ['the', 'system', 'is', 'composed', 'of', 'an', 'image', 'sensor', ',', 'placed', 'inside', 'the', 'cockpit', 'of', 'an', 'aircraft', 'to', 'record', 'images', 'of', 'the', 'agl', 'during', 'a', 'normal', 'descent', 'to', 'an', 'aerodrome', '.'], ['a', 'model-based', 'methodology', 'is', 'used', 'to', 'ascertain', 'the', 'optimum', 'match', 'between', 'a', 'template', 'of', 'the', 'agl', 'and', 'the', 'actual', 'image', 'data', 'in', 'order', 'to', 'calculate', 'the', 'position', 'and', 'orientation', 'of', 'the', 'camera', 'at', 'the', 'instant', 'the', 'image', 'was', 'acquired', '.'], ['the', 'camera', 'position', 'and', 'orientation', 'data', 'are', 'used', 'along', 'with', 'the', 'pixel', 'grey', 'level', 'for', 'each', 'imaged', 'luminaire', ',', 'to', 'estimate', 'a', 'value', 'for', 'the', 'luminous', 'intensity', 'of', 'a', 'given', 'luminaire', '.'], ['this', 'can', 'then', 'be', 'compared', 'with', 'the', 'expected', 'brightness', 'for', 'that', 'luminaire', 'to', 'ensure', 'it', 'is', 'operating', 'to', 'the', 'required', 'standards', '.'], ['as', 'such', ',', 'a', 'metric', 'for', 'the', 'quality', 'of', 'the', 'agl', 'pattern', 'is', 'determined', '.'], ['experiments', 'on', 'real', 'image', 'data', 'is', 'presented', 'to', 'demonstrate', 'the', 'application', 'and', 'effectiveness', 'of', 'the', 'system', '.']]\n",
      "Cosine similarity between 'development' and 'placed' - CBOW :  0.06146417\n",
      "recommendations\n",
      "[('each', 0.1920749545097351), ('that', 0.16941361129283905), ('data', 0.1678740680217743)]\n"
     ]
    }
   ],
   "source": [
    "import bs4 as bs\n",
    "import re\n",
    "import gensim\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "string=str(abstracts[0])\n",
    "data=[]\n",
    "# iterate through each sentence in the file\n",
    "for i in sent_tokenize(string):\n",
    "    temp = []\n",
    "     \n",
    "    # tokenize the sentence into words\n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j.lower())\n",
    " \n",
    "    data.append(temp)\n",
    "    \n",
    "# Create CBOW model\n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1,vector_size = 100, window = 5)\n",
    "print(data)\n",
    "print(\"Cosine similarity between 'development' \" +\"and 'placed' - CBOW : \", model1.wv.similarity(\"development\", \"placed\"))\n",
    "print(model1.wv.doesnt_match([\"development\", \"recommendations\",\"placed\" ]))#Word that doesn't match\n",
    "print(model1.wv.most_similar(positive=[\"development\", \"recommendations\", \"placed\"], negative=[\"inside\"], topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6df8b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in abstracts:\n",
    "    abstracts[node] = tuple(abstracts[node].split())\n",
    "    \n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#scaler = MinMaxScaler()\n",
    "#data_rescaled = scaler.fit_transform(str(abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac38fb3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Implement SentimentIntensityAnalyzer to find if the text is positive,negative or neutral\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "#nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "total=tuple()\n",
    "for i in range (0,n-1):\n",
    "    total+=abstracts[i]\n",
    "\n",
    "string=str(abstracts[0])    #For a quicker check\n",
    "#string=str(total)\n",
    "sia.polarity_scores(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4ab2a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(68654,\n",
       "  ('�',\n",
       "   'Although',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'fundamental',\n",
       "   'goals',\n",
       "   'of',\n",
       "   'AI',\n",
       "   'is',\n",
       "   'to',\n",
       "   'understand',\n",
       "   'and',\n",
       "   'develop',\n",
       "   'intelligent',\n",
       "   'systems',\n",
       "   'that',\n",
       "   'have',\n",
       "   'all',\n",
       "   'the',\n",
       "   'capabilities',\n",
       "   'of',\n",
       "   'humans,',\n",
       "   'there',\n",
       "   'is',\n",
       "   'little',\n",
       "   'active',\n",
       "   'research',\n",
       "   'directly',\n",
       "   'pursuing',\n",
       "   'this',\n",
       "   'goal.',\n",
       "   'We',\n",
       "   'propose',\n",
       "   'that',\n",
       "   'AI',\n",
       "   'for',\n",
       "   'interactive',\n",
       "   'computer',\n",
       "   'games',\n",
       "   'is',\n",
       "   'an',\n",
       "   'emerging',\n",
       "   'application',\n",
       "   'area',\n",
       "   'in',\n",
       "   'which',\n",
       "   'this',\n",
       "   'goal',\n",
       "   'of',\n",
       "   'human-level',\n",
       "   'AI',\n",
       "   'can',\n",
       "   'successfully',\n",
       "   'be',\n",
       "   'pursued.',\n",
       "   'Interactive',\n",
       "   'computer',\n",
       "   'games',\n",
       "   'have',\n",
       "   'increasingly',\n",
       "   'complex',\n",
       "   'and',\n",
       "   'realistic',\n",
       "   'worlds',\n",
       "   'and',\n",
       "   'increasingly',\n",
       "   'complex',\n",
       "   'and',\n",
       "   'intelligent',\n",
       "   'computer-controlled',\n",
       "   'characters.',\n",
       "   'In',\n",
       "   'this',\n",
       "   'article,',\n",
       "   'we',\n",
       "   'further',\n",
       "   'motivate',\n",
       "   'our',\n",
       "   'proposal',\n",
       "   'of',\n",
       "   'using',\n",
       "   'interactive',\n",
       "   'computer',\n",
       "   'games',\n",
       "   'for',\n",
       "   'AI',\n",
       "   'research,',\n",
       "   'review',\n",
       "   'previous',\n",
       "   'research',\n",
       "   'on',\n",
       "   'AI',\n",
       "   'and',\n",
       "   'games,',\n",
       "   'and',\n",
       "   'present',\n",
       "   'the',\n",
       "   'different',\n",
       "   'game',\n",
       "   'genres',\n",
       "   'and',\n",
       "   'the',\n",
       "   'roles',\n",
       "   'that',\n",
       "   'human-level',\n",
       "   'AI',\n",
       "   'could',\n",
       "   'play',\n",
       "   'within',\n",
       "   'these',\n",
       "   'genres.',\n",
       "   'We',\n",
       "   'then',\n",
       "   'describe',\n",
       "   'the',\n",
       "   'research',\n",
       "   'issues',\n",
       "   'and',\n",
       "   'AI',\n",
       "   'techniques',\n",
       "   'that',\n",
       "   'are',\n",
       "   'relevant',\n",
       "   'to',\n",
       "   'each',\n",
       "   'of',\n",
       "   'these',\n",
       "   'roles.',\n",
       "   'Our',\n",
       "   'conclusion',\n",
       "   'is',\n",
       "   'that',\n",
       "   'interactive',\n",
       "   'computer',\n",
       "   'games',\n",
       "   'provide',\n",
       "   'a',\n",
       "   'rich',\n",
       "   'environment',\n",
       "   'for',\n",
       "   'incremental',\n",
       "   'research',\n",
       "   'on',\n",
       "   'human-level',\n",
       "   'AI.')),\n",
       " (107423,\n",
       "  ('\\ufeff\\ufeff\\ufeff\\ufeff\\ufeffThe',\n",
       "   'Predication-based',\n",
       "   'Semantic',\n",
       "   'Indexing',\n",
       "   '(PSI)',\n",
       "   'approach',\n",
       "   'encodes',\n",
       "   'both',\n",
       "   'symbolic',\n",
       "   'and',\n",
       "   'distributional',\n",
       "   'information',\n",
       "   'into',\n",
       "   'a',\n",
       "   'semantic',\n",
       "   'space',\n",
       "   'using',\n",
       "   'a',\n",
       "   'permutation-based',\n",
       "   'variant',\n",
       "   'of',\n",
       "   'Random',\n",
       "   'Indexing.',\n",
       "   'In',\n",
       "   'this',\n",
       "   'paper,',\n",
       "   'we',\n",
       "   'develop',\n",
       "   'and',\n",
       "   'evaluate',\n",
       "   'a',\n",
       "   'computational',\n",
       "   'model',\n",
       "   'of',\n",
       "   'abductive',\n",
       "   'reasoning',\n",
       "   'based',\n",
       "   'on',\n",
       "   'PSI.',\n",
       "   'Using',\n",
       "   'distributional',\n",
       "   'information,',\n",
       "   'we',\n",
       "   'identify',\n",
       "   'pairs',\n",
       "   'of',\n",
       "   'concepts',\n",
       "   'that',\n",
       "   'are',\n",
       "   'likely',\n",
       "   'to',\n",
       "   'be',\n",
       "   'predicated',\n",
       "   'about',\n",
       "   'a',\n",
       "   'common',\n",
       "   'third',\n",
       "   'concept,',\n",
       "   'or',\n",
       "   'middle',\n",
       "   'term.',\n",
       "   'As',\n",
       "   'this',\n",
       "   'occurs',\n",
       "   'without',\n",
       "   'the',\n",
       "   'explicit',\n",
       "   'identification',\n",
       "   'of',\n",
       "   'the',\n",
       "   'middle',\n",
       "   'term',\n",
       "   'concerned,',\n",
       "   'we',\n",
       "   'refer',\n",
       "   'to',\n",
       "   'this',\n",
       "   'process',\n",
       "   'as',\n",
       "   'a',\n",
       "   '“logical',\n",
       "   'leap”.',\n",
       "   'Subsequently,',\n",
       "   'we',\n",
       "   'use',\n",
       "   'further',\n",
       "   'operations',\n",
       "   'in',\n",
       "   'the',\n",
       "   'PSI',\n",
       "   'space',\n",
       "   'to',\n",
       "   'retrieve',\n",
       "   'this',\n",
       "   'middle',\n",
       "   'term',\n",
       "   'and',\n",
       "   'identify',\n",
       "   'the',\n",
       "   'predicate',\n",
       "   'types',\n",
       "   'involved.',\n",
       "   'On',\n",
       "   'evaluation',\n",
       "   'using',\n",
       "   'a',\n",
       "   'set',\n",
       "   'of',\n",
       "   '1000',\n",
       "   'randomly',\n",
       "   'selected',\n",
       "   'cue',\n",
       "   'concepts,',\n",
       "   'the',\n",
       "   'model',\n",
       "   'is',\n",
       "   'shown',\n",
       "   'to',\n",
       "   'retrieve',\n",
       "   'with',\n",
       "   'accuracy',\n",
       "   'concepts',\n",
       "   'that',\n",
       "   'can',\n",
       "   'be',\n",
       "   'connected',\n",
       "   'to',\n",
       "   'a',\n",
       "   'cue',\n",
       "   'concept',\n",
       "   'by',\n",
       "   'a',\n",
       "   'middle',\n",
       "   'term,',\n",
       "   'as',\n",
       "   'well',\n",
       "   'as',\n",
       "   'the',\n",
       "   'middle',\n",
       "   'term',\n",
       "   'concerned,',\n",
       "   'using',\n",
       "   'nearest-neighbor',\n",
       "   'search',\n",
       "   'in',\n",
       "   'the',\n",
       "   'PSI',\n",
       "   'space.',\n",
       "   'The',\n",
       "   'utility',\n",
       "   'of',\n",
       "   'quantum',\n",
       "   'logical',\n",
       "   'operators',\n",
       "   'as',\n",
       "   'a',\n",
       "   'means',\n",
       "   'to',\n",
       "   'identify',\n",
       "   'alternative',\n",
       "   'paths',\n",
       "   'through',\n",
       "   'this',\n",
       "   'space',\n",
       "   'is',\n",
       "   'also',\n",
       "   'explored.')),\n",
       " (230,\n",
       "  ('\\uf020Concept',\n",
       "   'extraction',\n",
       "   'is',\n",
       "   'a',\n",
       "   'primary',\n",
       "   'subtask',\n",
       "   'of',\n",
       "   'ontology',\n",
       "   'construction.',\n",
       "   'It',\n",
       "   'is',\n",
       "   'difficult',\n",
       "   'to',\n",
       "   'extract',\n",
       "   'new',\n",
       "   'concepts',\n",
       "   'from',\n",
       "   'traditional',\n",
       "   'text',\n",
       "   'corpus.',\n",
       "   'Moreover,',\n",
       "   'building',\n",
       "   'a',\n",
       "   'single',\n",
       "   'ontology',\n",
       "   'for',\n",
       "   'multiple-topic',\n",
       "   'corpus',\n",
       "   'may',\n",
       "   'lead',\n",
       "   'to',\n",
       "   'misconception.',\n",
       "   'To',\n",
       "   'deal',\n",
       "   'with',\n",
       "   'these',\n",
       "   'problems,',\n",
       "   'this',\n",
       "   'paper',\n",
       "   'proposes',\n",
       "   'a',\n",
       "   'novel',\n",
       "   'framework',\n",
       "   'to',\n",
       "   'extract',\n",
       "   'topical',\n",
       "   'key',\n",
       "   'concepts',\n",
       "   'from',\n",
       "   'folksonomy.',\n",
       "   'Folksonomy',\n",
       "   'is',\n",
       "   'a',\n",
       "   'valuable',\n",
       "   'data',\n",
       "   'source',\n",
       "   'due',\n",
       "   'to',\n",
       "   'real-time',\n",
       "   'update',\n",
       "   'and',\n",
       "   'rich',\n",
       "   'user-generated',\n",
       "   'contents.',\n",
       "   'We',\n",
       "   'first',\n",
       "   'identify',\n",
       "   'topics',\n",
       "   'from',\n",
       "   'folksonomy',\n",
       "   'using',\n",
       "   'topic',\n",
       "   'models.',\n",
       "   'Next',\n",
       "   'the',\n",
       "   'tags',\n",
       "   'are',\n",
       "   'ranked',\n",
       "   'according',\n",
       "   'to',\n",
       "   'their',\n",
       "   'importance',\n",
       "   'for',\n",
       "   'a',\n",
       "   'certain',\n",
       "   'topic',\n",
       "   'by',\n",
       "   'applying',\n",
       "   'topic-specific',\n",
       "   'random',\n",
       "   'walk',\n",
       "   'methods.',\n",
       "   'The',\n",
       "   'top-ranking',\n",
       "   'tags',\n",
       "   'are',\n",
       "   'extracted',\n",
       "   'as',\n",
       "   'topical',\n",
       "   'key',\n",
       "   'concepts.',\n",
       "   'Especially,',\n",
       "   'a',\n",
       "   'novel',\n",
       "   'link',\n",
       "   'weight',\n",
       "   'function',\n",
       "   'which',\n",
       "   'combines',\n",
       "   'the',\n",
       "   'local',\n",
       "   'structure',\n",
       "   'information',\n",
       "   'and',\n",
       "   'global',\n",
       "   'semantic',\n",
       "   'similarity',\n",
       "   'is',\n",
       "   'proposed',\n",
       "   'in',\n",
       "   'importance',\n",
       "   'score',\n",
       "   'propagation.',\n",
       "   'From',\n",
       "   'the',\n",
       "   'perspectives',\n",
       "   'of',\n",
       "   'qualitative',\n",
       "   'and',\n",
       "   'quantitative',\n",
       "   'investigation,',\n",
       "   'our',\n",
       "   'method',\n",
       "   'is',\n",
       "   'feasible',\n",
       "   'and',\n",
       "   'effective.')),\n",
       " (44318,\n",
       "  ('•',\n",
       "   'We',\n",
       "   'proposed',\n",
       "   'a',\n",
       "   'simple',\n",
       "   'but',\n",
       "   'effective',\n",
       "   'offline',\n",
       "   'random',\n",
       "   'sampling',\n",
       "   'in',\n",
       "   'place',\n",
       "   'of',\n",
       "   'online',\n",
       "   'K-NN',\n",
       "   'search',\n",
       "   'to',\n",
       "   'improve',\n",
       "   'the',\n",
       "   'efficiency',\n",
       "   'of',\n",
       "   'face',\n",
       "   'sketch',\n",
       "   'synthesis.')),\n",
       " (55571,\n",
       "  ('•',\n",
       "   'We',\n",
       "   'present',\n",
       "   'and',\n",
       "   'train',\n",
       "   'a',\n",
       "   'neural',\n",
       "   'network',\n",
       "   'which',\n",
       "   'effectively',\n",
       "   'captures',\n",
       "   'the',\n",
       "   'semantics',\n",
       "   'of',\n",
       "   'context',\n",
       "   'and',\n",
       "   'learns',\n",
       "   'the',\n",
       "   'distributed',\n",
       "   'semantic',\n",
       "   'representations',\n",
       "   'of',\n",
       "   'two',\n",
       "   'kinds',\n",
       "   'of',\n",
       "   'contexts',\n",
       "   'for',\n",
       "   'recommendation.'))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "#print (fdist)\n",
    "fdist = FreqDist(abstracts)\n",
    "\n",
    "fdist1 = fdist.most_common(5)\n",
    "fdist1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a14dcf75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20300, 'Žiga Emeršič,Vitomir Štruc,Peter Peer\\n'),\n",
       " (97681,\n",
       "  'Želmira Tóthová,Jaroslav Polec,Tatiana Orgoniková,Lenka Krulikovská\\n'),\n",
       " (110528, 'Željko Agić,Barbara Plank,Anders Søgaard\\n'),\n",
       " (114532, 'Żeljko Agić,Nikola Ljubešić,Danijela Merkler\\n'),\n",
       " (51933, 'Żeljko Agić,Dirk Hovy,Anders Sogaard\\n')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors = dict()\n",
    "from collections import defaultdict\n",
    "with open('authors.txt', 'r',encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        node, author = line.split('|--|')\n",
    "        authors[int(node)] = author\n",
    "        \n",
    "        \n",
    "\n",
    "#Find the authors that are mostly in the list\n",
    "fdist = FreqDist(authors)\n",
    "\n",
    "fdist1 = fdist.most_common(5)\n",
    "fdist1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38750c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1449786a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Scaling by finding max and dividing each element by the max\\nfor i in range(0,m-1):\\n    for j in range(0,3):\\n        if (X_train[i,j]>max):\\n            max=X_train[i,j]\\n        \\nprint(max)\\n\\nfor i in range(0,m-1):\\n    for j in range(0,3):\\n        X_train[i,j]=X_train[i,j]/max\\n        \\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# its class label is 1 if it corresponds to an edge and 0, otherwise.\n",
    "# Use the following 3 features for each pair of nodes:\n",
    "# (1) sum of number of unique terms of the two nodes' abstracts\n",
    "# (2) absolute value of difference of number of unique terms of the two nodes' abstracts\n",
    "# (3) number of common terms between the abstracts of the two nodes\n",
    "X_train = np.zeros((2*m, 4))\n",
    "y_train = np.zeros(2*m)\n",
    "n = G.number_of_nodes()\n",
    "max=-1\n",
    "\n",
    "for i,edge in enumerate(G.edges()):\n",
    "    # an edge\n",
    "    X_train.at[i,0] = len(abstracts[edge[0]]) + len(abstracts[edge[1]])\n",
    "    X_train[i,1] = abs(len(abstracts[edge[0]]) - len(abstracts[edge[1]]))\n",
    "    #X_train[i,2] = len((abstracts[edge[0]]).intersection(abstracts[edge[1]]))\n",
    "    X_train[i,2]=len(list(set(abstracts[edge[0]])&set(abstracts[edge[1]])))\n",
    "    X_train[i,3]=len(abstracts[edge[0]])/(len(abstracts[edge[1]])+1)\n",
    "    #X_train[i,3]=abstracts[edge[0]]+abstracts[edge[1]]\n",
    "    y_train[i] = 1\n",
    "    \n",
    "    \n",
    "\n",
    "    # a randomly generated pair of nodes\n",
    "    n1 = randint(0, n-1)\n",
    "    n2 = randint(0, n-1)\n",
    "    X_train[m+i,0] = len(abstracts[n1]) + len(abstracts[n2])\n",
    "    X_train[m+i,1] = abs(len(abstracts[n1]) - len(abstracts[n2]))\n",
    "    #X_train[m+i,2] = len(abstracts[n1].intersection(abstracts[n2]))\n",
    "    X_train[m+i,2]=len(list(set(abstracts[n1])&set(abstracts[n2])))\n",
    "    X_train[m+i,3]=len(abstracts[n1])/(len(abstracts[n2])+1)\n",
    "    #X_train[m+i,3]=abstracts[edge[0]]+abstracts[edge[1]]\n",
    "    y_train[m+i] = 0\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "#Scaling by finding max and dividing each element by the max\n",
    "for i in range(0,m-1):\n",
    "    for j in range(0,3):\n",
    "        if (X_train[i,j]>max):\n",
    "            max=X_train[i,j]\n",
    "        \n",
    "print(max)\n",
    "\n",
    "for i in range(0,m-1):\n",
    "    for j in range(0,3):\n",
    "        X_train[i,j]=X_train[i,j]/max\n",
    "        \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74db58ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Feature selection on X_train\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X_train, y_train)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_train = model.transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "080f1379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training matrix: (2183910, 4)\n"
     ]
    }
   ],
   "source": [
    "print('Size of training matrix:', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "757c84ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((int(t[0]), int(t[1])))\n",
    "node_pairs=tuple(node_pairs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(node_pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15acfec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmax=-1\\nfor i in range (0,len(node_pairs)):\\n    for j in range (0,3):\\n        if (X_test[i,j]>max):\\n            max=X_test[i,j]\\n            \\nfor i in range(0,len(node_pairs)):\\n    for j in range(0,3):\\n        X_test[i,j]=X_test[i,j]/max\\n            \\nprint('Size of test matrix:', X_test.shape)\\n#X_test=pca.fit_transform(X_test)\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the test matrix. Use the same 4 features as above\n",
    "X_test = np.zeros((len(node_pairs), 4))\n",
    "for i,node_pair in enumerate(node_pairs):\n",
    "    X_test[i,0] = len(abstracts[node_pair[0]]) + len(abstracts[node_pair[1]])\n",
    "    X_test[i,1] = abs(len(abstracts[node_pair[0]]) - len(abstracts[node_pair[1]]))\n",
    "   # X_test[i,2] = len(abstracts[node_pair[0]].intersection(abstracts[node_pair[1]]))\n",
    "    X_test[i,2]=len(list(set(abstracts[node_pair[0]])&set(abstracts[node_pair[1]])))\n",
    "    X_test[i,3]=len(abstracts[node_pair[0]])/(len(abstracts[node_pair[1]])+1)\n",
    "    #X_test[i,3]=abstracts[node_pair[0]]+abstracts[node_pair[1]]\n",
    "    \n",
    "\"\"\"\n",
    "max=-1\n",
    "for i in range (0,len(node_pairs)):\n",
    "    for j in range (0,3):\n",
    "        if (X_test[i,j]>max):\n",
    "            max=X_test[i,j]\n",
    "            \n",
    "for i in range(0,len(node_pairs)):\n",
    "    for j in range(0,3):\n",
    "        X_test[i,j]=X_test[i,j]/max\n",
    "            \n",
    "print('Size of test matrix:', X_test.shape)\n",
    "#X_test=pca.fit_transform(X_test)\n",
    "\"\"\"\n",
    "\n",
    "#scaler=preprocessing.StandardScaler().fit(X_test)\n",
    "#X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a80ebd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2183910, 4)\n",
      "(2183910,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\"\"\"\n",
    "\n",
    "#Scale data\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "y_train=scaler.fit_transform(y_train.reshape(-1,1))\n",
    "X_test=scaler.fit_transform(X_test)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "#Apply PCA\n",
    "pca=PCA(n_components=0.99)\n",
    "X_train=pca.fit_transform(X_train,y_train)\n",
    "y_train=pca.fit_transform(y_train.reshape(-1, 1))\n",
    "X_test=pca.fit_transform(X_test)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#Apply LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA(n_components = 2)\n",
    "X_train = lda.fit_transform(X_train, y_train)\n",
    "X_test = lda.transform(X_test)\n",
    "\"\"\"\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03ebcd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "lab = preprocessing.LabelEncoder()\n",
    "y_train = lab.fit_transform(y_train.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15f2d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "clf = LogisticRegression(class_weight='balanced', max_iter=20,random_state=1234,solver='liblinear')\n",
    "#clf = MLPClassifier(hidden_layer_sizes=(6,5),\n",
    "                   # random_state=5,\n",
    "                    #verbose=True,\n",
    "                    #learning_rate_init=0.01)\n",
    "## ΝΑ ΔΟΚΙΜΑΣΩ ΚΑΙ RNNs,CNNs\n",
    "#clf=LogisticRegression()\n",
    "#clf=tree.DecisionTreeClassifier()\n",
    "start_time = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "print(\"Logistic Time: \", end_time-start_time, \"score: \", clf.score(X_train,y_train))\n",
    "\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "y_pred = y_pred[:,1]\n",
    "print(y_pred)\n",
    "\n",
    "print(\"Logistic R. score: \" , clf.score(X_train,y_train))\n",
    "print(classification_report(y_train,clf.predict(X_train)))\n",
    "print(confusion_matrix(y_train,clf.predict(X_train)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Bernoulli=BernoulliNB()\n",
    "Multinomial=MultinomialNB()\n",
    "Gaussian=GaussianNB()\n",
    "knn=KNeighborsClassifier(n_neighbors = 1)\n",
    "forest=RandomForestClassifier(n_estimators = 100) \n",
    "MLP= MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "Adaboost = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "SVClf = SVC(kernel='linear')\n",
    "LinearD=LDA()\n",
    "\n",
    "start_time = time.time()\n",
    "Bernoulli.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "print(\"Bernoulli Time: \", end_time-start_time, \" score: \" , Bernoulli.score(X_train,y_train))\n",
    "\n",
    "start_time = time.time()\n",
    "Multinomial.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "print(\"Multinomial Time: \", end_time-start_time,\" score: \" , Multinomial.score(X_train,y_train))\n",
    "\n",
    "start_time = time.time()\n",
    "Gaussian.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "print(\"Gaussian Time: \", end_time-start_time,\" score: \" , Gaussian.score(X_train,y_train))\n",
    "\n",
    "start_time = time.time()\n",
    "knn.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "print(\"KNN Time: \", end_time-start_time,\" score: \" , knn.score(X_train,y_train))\n",
    "\n",
    "start_time = time.time()\n",
    "forest.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "print(\"Random forest Time: \", end_time-start_time,\" score: \" , forest.score(X_train,y_train))\n",
    "\n",
    "start_time = time.time()\n",
    "MLP.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "print(\"MLP Time: \", end_time-start_time, \" score: \" , MLP.score(X_train,y_train))\n",
    "\n",
    "start_time = time.time()\n",
    "Adaboost.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "print(\"Adaboost Time: \", end_time-start_time,\" score: \" , Adaboost.score(X_train,y_train))\n",
    "\n",
    "start_time = time.time()\n",
    "SVClf.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "print(\"SVM Time: \", end_time-start_time,\" score: \" , SVClf.score(X_train,y_train))\n",
    "\n",
    "start_time = time.time()\n",
    "LinearD.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "print(\"LDA Time: \", end_time-start_time, \" score: \" , LinearD.score(X_train,y_train))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b8a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRIDSEARCHCV for MLP\n",
    "\n",
    "from sklearn.metrics import make_scorer,fbeta_score\n",
    "GRID = [\n",
    "    {'scaler': [StandardScaler()],\n",
    "     'estimator': [MLPClassifier(random_state=101)],\n",
    "     'estimator__solver': ['adam'],\n",
    "     'estimator__learning_rate_init': [0.0001],\n",
    "     'estimator__max_iter': [300],\n",
    "     #'estimator__hidden_layer_sizes': [(500, 400, 300, 200, 100), (400, 400, 400, 400, 400), (300, 300, 300, 300, 300), (200, 200, 200, 200, 200)],\n",
    "     'estimator__activation': ['logistic', 'tanh', 'relu'],\n",
    "     'estimator__alpha': [ 0.001, 0.005]\n",
    "    # 'estimator__early_stopping': [True, False]\n",
    "     }\n",
    "]\n",
    "\n",
    "PIPELINE = Pipeline([('scaler', None), ('estimator', MLPClassifier())])\n",
    "\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(estimator=PIPELINE, param_grid=GRID, \n",
    "                            scoring=make_scorer(fbeta_score, beta=2),# average='macro'), \n",
    "                            n_jobs=-1, refit=True, verbose=1, \n",
    "                            return_train_score=False)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.score(X_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d6f0779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n",
      "Best score for the model after tuning is:  0.7177850735607236\n",
      "Best parameters for the model is : LogisticRegression(class_weight='balanced', max_iter=50, random_state=1234,\n",
      "                   solver='liblinear')\n"
     ]
    }
   ],
   "source": [
    "#GridSearchCV for logistic regression\n",
    "parameter_grid_logistic_regression = {\n",
    "    'max_iter': [ 20,50],                      # Number of iterations\n",
    "    'solver': [ 'liblinear', 'saga'],   # Algorithm to use for optimization\n",
    "    'class_weight': ['balanced']                                    # Troubleshoot unbalanced data sampling\n",
    "} \n",
    "\n",
    "\n",
    "logistic_Model_grid = GridSearchCV(estimator=LogisticRegression(random_state=1234), param_grid=parameter_grid_logistic_regression, verbose=1, \n",
    "                    cv=10, n_jobs=-1)\n",
    "\n",
    "logistic_Model_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best score for the model after tuning is: \",logistic_Model_grid.best_score_)\n",
    "print(\"Best parameters for the model is :\",logistic_Model_grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4316005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = forest.predict_proba(X_test)\n",
    "y_pred = y_pred[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de501b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions to a file\n",
    "predictions = zip(range(len(y_pred)), y_pred)\n",
    "with open(\"submission_text.csv\",\"w\") as pred:\n",
    "    csv_out = csv.writer(pred)\n",
    "    csv_out.writerow(['id','predicted'])\n",
    "    for row in predictions:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a37526c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
